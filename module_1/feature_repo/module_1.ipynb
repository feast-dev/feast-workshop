{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Serving fresh online features with Feast, Kafka, Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "In this notebook, we explore why it's valuable to build / register streaming features in Feast for both data scientists and engineers. \n",
    "\n",
    "We then use Spark to build streaming features from events in Kafka and registering them within Feast. We then showcase how Feast combines these streaming features with batch data sources in the online store (Redis). Users can then retrieve features at low latency from Redis through Feast.\n",
    "\n",
    "If you haven't already, look at the [README](../README.md) for setup instructions prior to starting this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../architecture.png\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Spark Structured Streaming to read this Kafka Topic\n",
    "We first read and parse events from Kafka, run some transformations with Spark, and `forEachBatch` push to Feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/dannychiao/.pyenv/versions/3.8.10/envs/python-3.8/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/dannychiao/.pyenv/versions/3.8.10/envs/python-3.8/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/dannychiao/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/dannychiao/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0d9ed2b9-7a12-469e-9d36-33faad4195e7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 415ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0d9ed2b9-7a12-469e-9d36-33faad4195e7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/16ms)\n",
      "22/05/18 12:21:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"feast-spark\").getOrCreate()\n",
    "# Reduce partitions since default is 200 which will be slow on a local machine\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "schema = (\n",
    "    StructType()\n",
    "        .add('driver_id', IntegerType(), False)\n",
    "        .add('miles_driven', DoubleType(), False)\n",
    "        .add('event_timestamp', TimestampType(), False)\n",
    "        .add('conv_rate', DoubleType(), False)\n",
    "        .add('acc_rate', DoubleType(), False)\n",
    ")\n",
    "\n",
    "# Subscribe to 1 topic, with headers\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"drivers\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .selectExpr('CAST(value AS STRING)')\n",
    "    .select(from_json('value', schema).alias(\"temp\"))\n",
    "    .select(\"temp.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup the feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply feature repository\n",
    "We first run `feast apply` to register the data sources + features and setup Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created entity \u001b[1m\u001b[32mdriver\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mdriver_daily_features\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n",
      "Created feature service \u001b[1m\u001b[32mmodel_v2\u001b[0m\n",
      "Created feature service \u001b[1m\u001b[32mmodel_v1\u001b[0m\n",
      "\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_daily_features\u001b[0m\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate a Feast `FeatureStore` object to push data to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "from datetime import datetime\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch training data from offline store\n",
    "Just to verify the features are in the batch sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      driver_id                  event_timestamp  conv_rate  acc_rate  \\\n",
      "360        1001        2021-04-12 10:59:42+00:00   0.521149  0.751659   \n",
      "721        1002        2021-04-12 08:12:10+00:00   0.089014  0.212637   \n",
      "1084       1003        2021-04-12 16:40:26+00:00   0.188855  0.344736   \n",
      "1445       1004        2021-04-12 15:01:12+00:00   0.296492  0.935305   \n",
      "1805       1001 2022-05-18 12:21:21.773880+00:00   0.404588  0.407571   \n",
      "\n",
      "      daily_miles_driven  \n",
      "360            18.926695  \n",
      "721            12.005569  \n",
      "1084           23.490234  \n",
      "1445           19.204191  \n",
      "1805          350.650257  \n"
     ]
    }
   ],
   "source": [
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001, 1002, 1003, 1004, 1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\"\n",
    "    ],\n",
    ").to_df()\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Materialize batch features & fetch online features from Redis\n",
    "First we materialize features (which generate the latest values for each entity key from batch sources) into the online store (Redis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views to \u001b[1m\u001b[32m2022-05-17 20:00:00-04:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mdriver_daily_features\u001b[0m from \u001b[1m\u001b[32m1748-08-02 16:21:29-04:56:02\u001b[0m to \u001b[1m\u001b[32m2022-05-17 20:00:00-04:00\u001b[0m:\n",
      "100%|████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 443.50it/s]\n",
      "\u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m from \u001b[1m\u001b[32m1748-08-02 16:21:29-04:56:02\u001b[0m to \u001b[1m\u001b[32m2022-05-17 20:00:00-04:00\u001b[0m:\n",
      "100%|████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 886.71it/s]\n"
     ]
    }
   ],
   "source": [
    "!feast materialize-incremental $(date +%Y-%m-%d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feast manages what time intervals have been materialized in the registry. So if you schedule regular materialization every hour, you can run `feast materialize-incremental` and Feast will know that all the previous hours were already processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDK based online retrieval\n",
    "Now we can retrieve these materialized features from Redis by directly using the SDK. This is one of the most popular ways to retrieve features with Feast since it allows you to integrate with an existing service (e.g. a Flask) that also handles model inference or pre/post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [350.6502685546875]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP based online retrieval\n",
    "We can also retrieve from a deployed feature server. We had previously deployed this with Docker Compose (see [docker-compose.yml](../docker-compose.yml))\n",
    "\n",
    "This can be preferable for many reasons. If you want to build an in-memory cache, caching on a central feature server can allow more effective caching across teams. You can also more centrally manage rate-limiting / access control, upgrade Feast versions independently, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"metadata\": {\n",
      "        \"feature_names\": [\n",
      "            \"driver_id\",\n",
      "            \"acc_rate\",\n",
      "            \"conv_rate\"\n",
      "        ]\n",
      "    },\n",
      "    \"results\": [\n",
      "        {\n",
      "            \"event_timestamps\": [\n",
      "                \"1970-01-01T00:00:00Z\"\n",
      "            ],\n",
      "            \"statuses\": [\n",
      "                \"PRESENT\"\n",
      "            ],\n",
      "            \"values\": [\n",
      "                1001\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"event_timestamps\": [\n",
      "                \"2022-03-14T14:00:00Z\"\n",
      "            ],\n",
      "            \"statuses\": [\n",
      "                \"PRESENT\"\n",
      "            ],\n",
      "            \"values\": [\n",
      "                0.4075707495212555\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"event_timestamps\": [\n",
      "                \"2022-03-14T14:00:00Z\"\n",
      "            ],\n",
      "            \"statuses\": [\n",
      "                \"PRESENT\"\n",
      "            ],\n",
      "            \"values\": [\n",
      "                0.4045884609222412\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "online_request = {\n",
    "  \"features\": [\n",
    "    \"driver_hourly_stats:conv_rate\",\n",
    "    \"driver_hourly_stats:acc_rate\"\n",
    "  ],\n",
    "  \"entities\": {\n",
    "    \"driver_id\": [1001]\n",
    "  }\n",
    "}\n",
    "r = requests.post('http://localhost:6566/get-online-features', data=json.dumps(online_request))\n",
    "print(json.dumps(r.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating fresher features via stream transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Building streaming features with Kafka + Spark Structured Streaming\n",
    "Now we push streaming features into Feast by ingesting events from Kafka and processing with Spark Structured Streaming.\n",
    "- These features can then be further post-processed and combined with other features or request data in on demand transforms.\n",
    "- An example might be to push in the last 5 transactions, and in on demand transforms generate the average of those transactions.\n",
    "\n",
    "Feast will help manage both batch and streaming sources for you. You can run `feast materialize-incremental` as well as ingest streaming features to the same online store.\n",
    "\n",
    "**Below, what's happening:**\n",
    "- We use Spark to compute a sliding window aggregate feature that computes `daily_miles_driven` using the `miles_driven` column in the event.\n",
    "- Triggers every 30 seconds\n",
    "    - In this case, because we’re just reading from the `driver_stats.parquet`, we could have multiple windows of data coming in. Thus, in this code, we filter for the latest (`driver_id`, `window`) feature\n",
    "    - If you have a larger watermark and can get events across multiple windows, you’ll want to have the latest window too.\n",
    "- Rename `end` to `event_timestamp`, otherwise Feast will throw a validation error since it doesn’t match the schema of the `FeatureView`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 12:22:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/05/18 12:22:23 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:22:23 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:22:23 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:22:23 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:22:23 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "/Users/dannychiao/.pyenv/versions/3.8.10/envs/python-3.8/lib/python3.8/site-packages/feast/feature_store.py:1219: RuntimeWarning: Push source is an experimental feature. This API is unstable and it could and might change in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "              event_timestamp  daily_miles_driven  \\\n",
      "driver_id                                           \n",
      "1001      2023-03-02 17:00:00          430.780262   \n",
      "1002      2023-03-02 18:00:00          665.962641   \n",
      "1003      2023-03-02 18:00:00          628.753097   \n",
      "1004      2023-03-02 17:00:00          650.300664   \n",
      "1005      2023-03-02 17:00:00          560.144795   \n",
      "\n",
      "                                   created  \n",
      "driver_id                                   \n",
      "1001      2022-05-18 16:22:23.684010+00:00  \n",
      "1002      2022-05-18 16:22:23.684010+00:00  \n",
      "1003      2022-05-18 16:22:23.684010+00:00  \n",
      "1004      2022-05-18 16:22:23.684010+00:00  \n",
      "1005      2022-05-18 16:22:23.684010+00:00  \n",
      "Num rows: 5\n",
      "processing window\n",
      "              event_timestamp  daily_miles_driven  \\\n",
      "driver_id                                           \n",
      "1001      2023-03-03 12:00:00          631.901269   \n",
      "1002      2023-03-03 12:00:00          612.062026   \n",
      "1003      2023-03-03 12:00:00          407.720938   \n",
      "1004      2023-03-03 12:00:00          598.710417   \n",
      "1005      2023-03-03 12:00:00          542.253628   \n",
      "\n",
      "                                   created  \n",
      "driver_id                                   \n",
      "1001      2022-05-18 16:22:34.060287+00:00  \n",
      "1002      2022-05-18 16:22:34.060287+00:00  \n",
      "1003      2022-05-18 16:22:34.060287+00:00  \n",
      "1004      2022-05-18 16:22:34.060287+00:00  \n",
      "1005      2022-05-18 16:22:34.060287+00:00  \n",
      "Num rows: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannychiao/.pyenv/versions/3.8.10/envs/python-3.8/lib/python3.8/site-packages/feast/feature_store.py:1219: RuntimeWarning: Push source is an experimental feature. This API is unstable and it could and might change in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def send_to_feast(df, epoch):\n",
    "    pandas_df: pd.DataFrame = df.toPandas()\n",
    "    if pandas_df.empty:\n",
    "        return\n",
    "    \n",
    "    if \"end\" in pandas_df:\n",
    "        print(\"processing window\")\n",
    "        # Filter out only for the latest window for the driver id\n",
    "        pandas_df = pandas_df.sort_values(by=[\"driver_id\",\"end\"], ascending=False).groupby(\"driver_id\").nth(-1)\n",
    "        pandas_df = pandas_df.rename(columns = {\"end\": \"event_timestamp\"})\n",
    "        pandas_df['created'] = pd.to_datetime('now', utc=True)\n",
    "        store.push(\"driver_stats_push_source\", pandas_df)\n",
    "    pandas_df.sort_values(by=\"driver_id\", inplace=True)\n",
    "    print(pandas_df.head(20))\n",
    "    print(f\"Num rows: {len(pandas_df.index)}\")\n",
    "\n",
    "daily_miles_driven = (\n",
    "    df.withWatermark(\"event_timestamp\", \"1 second\") \n",
    "        .groupBy(\"driver_id\", window(timeColumn=\"event_timestamp\", windowDuration=\"1 day\", slideDuration=\"1 hour\"))\n",
    "        .agg(sum(\"miles_driven\").alias(\"daily_miles_driven\"))\n",
    "        .select(\"driver_id\", \"window.end\", \"daily_miles_driven\")\n",
    ")\n",
    "\n",
    "query_1 = daily_miles_driven \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/feast-workshop/q1/\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .foreachBatch(send_to_feast) \\\n",
    "    .start()\n",
    "\n",
    "query_1.awaitTermination(timeout=30)\n",
    "query_1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b. Verify fresh features\n",
    "Now we can verify that the `daily_miles_driven` feature has indeed changed from the original materialized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [631.9012451171875]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5c (optional): Push features through the push server\n",
    "We could also have pushed features without a `FeatureStore` object (and hence the Spark jobs don't need a `feature_store.yaml`) and instead through an HTTP request to the push server (`requests.post(\"http://localhost:6567/push\", data=json.dumps(push_data))`) \n",
    "\n",
    "We already deployed a push server with Docker Compose at port 6567. Separating writes (pushes) from reads (feature serving) can be helpful for independent scaling + resource isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 12:23:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/05/18 12:23:06 WARN StreamingQueryManager: Stopping existing streaming query [id=c1245bc6-365d-4770-9172-1fb403ccb4f6, runId=61ed0b3f-2948-4170-8187-5ffd4e577502], as a new run is being started.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "   driver_id      event_timestamp  daily_miles_driven  \\\n",
      "0       1001  2023-03-08 13:00:00          596.305352   \n",
      "1       1002  2023-03-08 14:00:00          644.051849   \n",
      "2       1003  2023-03-08 14:00:00          547.179926   \n",
      "3       1004  2023-03-08 14:00:00          695.276622   \n",
      "4       1005  2023-03-08 14:00:00          640.746719   \n",
      "\n",
      "                            created  \n",
      "0  2022-05-18 16:23:10.699456+00:00  \n",
      "1  2022-05-18 16:23:10.699456+00:00  \n",
      "2  2022-05-18 16:23:10.699456+00:00  \n",
      "3  2022-05-18 16:23:10.699456+00:00  \n",
      "4  2022-05-18 16:23:10.699456+00:00  \n",
      "Num rows: 5\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def send_to_feast(df, epoch):\n",
    "    pandas_df: pd.DataFrame = df.toPandas()\n",
    "    if pandas_df.empty:\n",
    "        return\n",
    "    \n",
    "    if \"end\" in pandas_df:\n",
    "        print(\"processing window\")\n",
    "        # Filter out only for the latest window for the driver id\n",
    "        pandas_df = pandas_df.sort_values(by=[\"driver_id\",\"end\"], ascending=False).groupby(\"driver_id\").nth(-1)\n",
    "        pandas_df = pandas_df.rename(columns = {\"end\": \"event_timestamp\"})\n",
    "        pandas_df['created'] = pd.to_datetime('now', utc=True)\n",
    "        pandas_df.reset_index(inplace=True)\n",
    "\n",
    "        # Push the event via HTTP (we need to map timestamps to strings since timestamps aren't serializable)\n",
    "        pandas_df['created'] = pandas_df['created'].astype(str)\n",
    "        pandas_df['event_timestamp'] = pandas_df['event_timestamp'].astype(str)\n",
    "        pandas_dict = pandas_df.to_dict(orient=\"list\")\n",
    "        push_data = {\n",
    "            \"push_source_name\":\"driver_stats_push_source\",\n",
    "            \"df\":pandas_dict\n",
    "        }\n",
    "        requests.post(\"http://localhost:6567/push\", data=json.dumps(push_data))\n",
    "    pandas_df.sort_values(by=\"driver_id\", inplace=True)\n",
    "    print(pandas_df.head(20))\n",
    "    print(f\"Num rows: {len(pandas_df.index)}\")\n",
    "\n",
    "query_2 = daily_miles_driven \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/feast-workshop/q2/\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .foreachBatch(send_to_feast) \\\n",
    "    .start()\n",
    "\n",
    "query_2.awaitTermination(timeout=20)\n",
    "query_2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify yet again that features are more fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [596.3053588867188]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Finally, let's clean up the checkpoint directories from Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_path = '/tmp/feast-workshop/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d634b9af180bcb32a446a43848522733ff8f5bbf0cc46dba1a83bede04bf237"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('python-3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
